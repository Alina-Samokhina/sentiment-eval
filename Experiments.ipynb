{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T03:04:46.740590Z",
     "start_time": "2020-05-23T03:04:44.390498Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T03:04:48.545404Z",
     "start_time": "2020-05-23T03:04:48.528340Z"
    }
   },
   "outputs": [],
   "source": [
    "curr_dir = Path('.').resolve()\n",
    "twitter_dir = curr_dir / \"SentiRuEval_twitter\"\n",
    "reviews_dir = curr_dir / \"SentiRuEval-2015\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:12:29.328043Z",
     "start_time": "2020-05-23T02:12:29.320212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:17:12.782445Z",
     "start_time": "2020-05-23T02:12:29.329189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes up to 10 minutes  \n",
    "gen_ft = utils.get_pretrained_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:08.562012Z",
     "start_time": "2020-05-23T02:17:12.784263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/GitHub/sentiment-eval/utils/data_utils.py:84: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  aspects_df = aspects_df[['review_id', 'term', 'sentiment', 'category', 'from', 'to']][aspects_df['type']=='explicit'][aspects_df['mark']=='Rel']\n"
     ]
    }
   ],
   "source": [
    "ds_train = utils.get_dataset(reviews_dir/\"SentiRuEval_rest_markup_train.xml\")\n",
    "ds_test = utils.get_dataset(reviews_dir / \"SentiRuEval_rest_markup_test.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here is normalized: punctuation is thrown away, text is lowered and reduced to \"normal form\" using pymorphy2 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:08.588270Z",
     "start_time": "2020-05-23T02:18:08.564433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>categories</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>mask_asp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[я, быть, здесь, с, подруга, пока, один, раз, ...</td>\n",
       "      <td>[живую музыку, Заказали столик, первом этаже, ...</td>\n",
       "      <td>[Interior, Service, Interior, Whole, Interior,...</td>\n",
       "      <td>[positive, neutral, neutral, neutral, neutral,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>[хотеться, поделиться, с, весь, пользователь, ...</td>\n",
       "      <td>[ресторане Duplex, ресторан, ресторан, качеств...</td>\n",
       "      <td>[Whole, Whole, Whole, Food, Interior, Interior...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, neutral, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[отличный, кафе, отмечать, в, немой, свой, ден...</td>\n",
       "      <td>[кафе, еда, блюда, обстановка, администратор, ...</td>\n",
       "      <td>[Whole, Food, Food, Interior, Service, Whole, ...</td>\n",
       "      <td>[positive, positive, positive, positive, posit...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "99   [я, быть, здесь, с, подруга, пока, один, раз, ...   \n",
       "184  [хотеться, поделиться, с, весь, пользователь, ...   \n",
       "20   [отличный, кафе, отмечать, в, немой, свой, ден...   \n",
       "\n",
       "                                               aspects  \\\n",
       "99   [живую музыку, Заказали столик, первом этаже, ...   \n",
       "184  [ресторане Duplex, ресторан, ресторан, качеств...   \n",
       "20   [кафе, еда, блюда, обстановка, администратор, ...   \n",
       "\n",
       "                                            categories  \\\n",
       "99   [Interior, Service, Interior, Whole, Interior,...   \n",
       "184  [Whole, Whole, Whole, Food, Interior, Interior...   \n",
       "20   [Whole, Food, Food, Interior, Service, Whole, ...   \n",
       "\n",
       "                                            sentiments  \\\n",
       "99   [positive, neutral, neutral, neutral, neutral,...   \n",
       "184  [neutral, neutral, neutral, neutral, neutral, ...   \n",
       "20   [positive, positive, positive, positive, posit...   \n",
       "\n",
       "                                              mask_asp  \n",
       "99   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "184  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "20   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:08.930331Z",
     "start_time": "2020-05-23T02:18:08.768472Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = np.unique(np.hstack(ds_train.mask_asp.values), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three possible results for each word: \"aspect (first word)\", \"non-aspect\", \"aspect (not first word)\". Let's see if these classes are balanced, which they're probably not. And calculate weights for them, if they're unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:09.043679Z",
     "start_time": "2020-05-23T02:18:08.936531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.88%,\n",
      "1: 0.10%,\n",
      "2: 0.01%\n"
     ]
    }
   ],
   "source": [
    "print(f'0: {counts[1][0]/np.sum(counts[1]):.2f}%,\\n1: {counts[1][1]/np.sum(counts[1]):.2f}%,\\n2: {counts[1][2]/np.sum(counts[1]):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:09.273035Z",
     "start_time": "2020-05-23T02:18:09.045640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1235955056179776, 10.0, 100.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weights:\n",
    "1/0.89, 1/0.1, 1/0.01    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words of these classes differs significantly, so we will calculate our loss function weighed accordingly:\n",
    "\n",
    " - \"0\" with 1.2\n",
    " - \"1\" with 10\n",
    " - \"2\" with 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our vocabulary is ok for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:16.496123Z",
     "start_time": "2020-05-23T02:18:09.275699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.67% unknown words. examples:\n",
      " [0, '250р', 'albertone', 'банкеть', 'вразный', 'девчёнка', 'закусочка', 'кафитерия', 'марчеллис', 'необычненький', 'оринтироваться', 'панакот', 'приветсвый', 'сумотоха', 'халапена']\n"
     ]
    }
   ],
   "source": [
    "unks = [0]\n",
    "for word in utils.get_ds_vocab(ds_train):\n",
    "    if word not in gen_ft.index2entity:\n",
    "        unks.append(word)\n",
    "print(f'{len(unks)/len(utils.get_ds_vocab(ds_train))*100:.2f}% unknown words. examples:\\n', unks[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for fasttext from wiki & lenta < 5% unknown, that's ok, I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:16.775036Z",
     "start_time": "2020-05-23T02:18:16.497961Z"
    }
   },
   "outputs": [],
   "source": [
    "voc_ft = dict(zip(gen_ft.index2entity, range(len(gen_ft.index2entity))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:39.667767Z",
     "start_time": "2020-05-23T02:18:39.515000Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential = True, use_vocab = False, batch_first=True, unk_token = 977837, pad_token = 977838) \n",
    "TARGET = Field(sequential = True, use_vocab = False, batch_first=True, is_target = True, unk_token = 3, pad_token = 4)\n",
    "train_ds = utils.DataFrameDataset(utils.text2ind(ds_train, voc_ft), {'text': TEXT, 'mask_asp': TARGET})\n",
    "valid_ds = utils.DataFrameDataset(utils.text2ind(ds_test, voc_ft), {'text': TEXT, 'mask_asp': TARGET})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:42.475889Z",
     "start_time": "2020-05-23T02:18:42.463398Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    "        (train_ds, valid_ds),\n",
    "        batch_sizes=(64, 64),\n",
    "        sort=True,\n",
    "        sort_key= lambda x: len(x.text),\n",
    "        sort_within_batch=False,\n",
    "        device=device,\n",
    "        repeat=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:48.124906Z",
     "start_time": "2020-05-23T02:18:48.111947Z"
    }
   },
   "outputs": [],
   "source": [
    "weights_ft = torch.FloatTensor(gen_ft.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-23T02:42:44.616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "_lP_lNIzSK7d",
    "outputId": "7a249188-67a5-45f3-b2f5-675e36dd076b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dom_emb_dim = 100\n",
    "voc_size = weights_ft.shape[0]\n",
    "hid_dim = 200\n",
    "encoder = utils.EncoderRNN(hid_dim, dom_emb_dim, voc_size,weights_ft )\n",
    "encoder.to(device)\n",
    "decoder = utils.DecoderRNN(hid_dim, dom_emb_dim, voc_size, weights_ft, output_dim = 5)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-23T02:42:45.040Z"
    }
   },
   "outputs": [],
   "source": [
    "model = utils.Seq2Seq(encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-23T02:42:45.481Z"
    }
   },
   "outputs": [],
   "source": [
    "model(batch.text, batch.mask_asp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC3jGjlnSK7n"
   },
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:17.029607Z",
     "start_time": "2020-05-23T02:12:27.046Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "s372iG_lSK7p"
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss(weight = torch.tensor([1., 10, 10, 10, 10]), ignore_index = 4)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T21:08:30.773203Z",
     "start_time": "2020-05-22T21:08:30.507093Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "N0bEeSoOSK7z",
    "outputId": "a0a8a66e-9ca7-489d-a319-b48ddf6275ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.4911468625068665, Validation Loss: 1.3939503133296967\n",
      "Epoch: 2, Training Loss: 1.320342093706131, Validation Loss: 1.291944146156311\n",
      "Epoch: 3, Training Loss: 1.2084271609783173, Validation Loss: 1.2319333255290985\n",
      "Epoch: 4, Training Loss: 1.1336623132228851, Validation Loss: 1.1764418184757233\n",
      "Epoch: 5, Training Loss: 1.0854232609272003, Validation Loss: 1.1433232128620148\n",
      "Epoch: 6, Training Loss: 1.029188871383667, Validation Loss: 1.1067364513874054\n",
      "Epoch: 7, Training Loss: 1.0009105801582336, Validation Loss: 1.080284595489502\n",
      "Epoch: 8, Training Loss: 0.9670122116804123, Validation Loss: 1.036013275384903\n",
      "Epoch: 9, Training Loss: 0.9518527686595917, Validation Loss: 1.0009718835353851\n",
      "Epoch: 10, Training Loss: 0.9164797365665436, Validation Loss: 1.0004175454378128\n",
      "Epoch: 11, Training Loss: 0.895934596657753, Validation Loss: 0.9864820688962936\n",
      "Epoch: 12, Training Loss: 0.8673616349697113, Validation Loss: 0.9540105909109116\n",
      "Epoch: 13, Training Loss: 0.8760020285844803, Validation Loss: 0.9468487203121185\n",
      "Epoch: 14, Training Loss: 0.8521246910095215, Validation Loss: 0.9560002237558365\n",
      "Epoch: 15, Training Loss: 0.8466178923845291, Validation Loss: 0.9378547519445419\n",
      "Epoch: 16, Training Loss: 0.8458617627620697, Validation Loss: 0.9375575184822083\n",
      "Epoch: 17, Training Loss: 0.8192343711853027, Validation Loss: 0.9180964380502701\n",
      "Epoch: 18, Training Loss: 0.8058288842439651, Validation Loss: 0.9500723630189896\n",
      "Epoch: 19, Training Loss: 0.8194858878850937, Validation Loss: 0.9084072560071945\n",
      "Epoch: 20, Training Loss: 0.8003931939601898, Validation Loss: 0.91804638504982\n",
      "Epoch: 21, Training Loss: 0.8225317001342773, Validation Loss: 0.9173529893159866\n",
      "Epoch: 22, Training Loss: 0.7993368208408356, Validation Loss: 0.8977751135826111\n",
      "Epoch: 23, Training Loss: 0.8197789937257767, Validation Loss: 0.894617572426796\n",
      "Epoch: 24, Training Loss: 0.809046283364296, Validation Loss: 0.8877096921205521\n",
      "Epoch: 25, Training Loss: 0.8075473308563232, Validation Loss: 0.9019636511802673\n",
      "Epoch: 26, Training Loss: 0.8163838088512421, Validation Loss: 0.8951563835144043\n",
      "Epoch: 27, Training Loss: 0.8142554461956024, Validation Loss: 0.9100414961576462\n",
      "Epoch: 28, Training Loss: 0.8141392916440964, Validation Loss: 0.9023299813270569\n",
      "Epoch: 29, Training Loss: 0.8108140528202057, Validation Loss: 0.8912506401538849\n",
      "Epoch: 30, Training Loss: 0.7939941138029099, Validation Loss: 0.8968428075313568\n",
      "CPU times: user 10min 53s, sys: 14.9 s, total: 11min 8s\n",
      "Wall time: 11min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        src = batch.text\n",
    "        trg = batch.mask_asp\n",
    "\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output = model(src, trg, 0.7)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        loss = loss_func(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_iter)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        src = batch.text\n",
    "        trg = batch.mask_asp\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        loss = loss_func(output, trg)\n",
    "        \n",
    "        val_loss += loss.item()  \n",
    "        \n",
    "    val_loss /= len(val_iter)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T02:18:17.032616Z",
     "start_time": "2020-05-23T02:12:27.059Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PR0-meKfyk_n"
   },
   "outputs": [],
   "source": [
    "batch = next(val_iter.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "nBMChvRFd0xw",
    "outputId": "2b319afc-0ce6-4b1d-b00a-03a9fbd4b488"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 4, 4, 4],\n",
       "        [0, 0, 1,  ..., 4, 4, 4],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 4, 4, 4],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 146,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.mask_asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMGqI3oDd2Mu"
   },
   "outputs": [],
   "source": [
    "results = torch.argmax(F.softmax(model(batch.text, torch.LongTensor(64, batch.mask_asp.shape[1]).fill_(4) , 0.), dim = 2), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "NhhqBvemd3xK",
    "outputId": "05e34265-5090-41e4-a12f-6eed0107bcb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1,\n",
       "        2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 157,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What has been done so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My team (and also I as part of that team) wasn't very organized. So, what I thought was to be done by 4 people, I ended up doing alone. I don't blame anyone, but that's what I've got so far on my own.\n",
    "\n",
    "1. *understanding the task*.\n",
    "\n",
    "   I have no prior experience other than our homeworks in this kind of tasks, so it took a lot of time just to understand the purpose and the structure of the problem. I googled some methods and decided to start with some simple ones so that process of debugging wouldn't be too exhausting. And because big model require much more data to be trained.\n",
    "   \n",
    "   \n",
    "2. *data preparation* \n",
    "\n",
    "    To understand in what format the data should be stored, how to parse it from xml and so on also took more than a few days. Found pretrained embeddings (our corpus is not big enough to obtain good ones with an at leas ok number of words)\n",
    "    \n",
    "    \n",
    "3. *coding*\n",
    "\n",
    "    That part was the most fun: trying to remember how seq2seq worked and how to use attention there. Some ideas to try out little bit later:\n",
    "    - use not general word collection (pre-trained on wiki), but pretrained on twitter. \n",
    "    - use BERT for embeddings\n",
    "    - use more sophisticated approaches than vanilla seq2seq\n",
    "    - maybe add some attention here (?)\n",
    "    \n",
    "   \n",
    "    \n",
    "4. *results*\n",
    "\n",
    "    My model sucks. It is learning. But either it has too few data to train on, or it is just not good enough for this task. Also different weights in loss gives different results, but I couldn't find the optimal ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part was supposed to be the first step to solve the task of aspect based sentiment analysis.\n",
    "\n",
    "1. Aspect extraction [Done]\n",
    "2. Aspect categorization [To-do]\n",
    "3. Understanding sentiment toward each aspect [To-do]\n",
    "4. Summarize sentiments to aspects inside each category, report on sentiment of each category. [To-do]\n",
    "\n",
    "\n",
    "Second task is simple classification. For each aspect, using embedding, bigrams and rhreegrams we'll do CNN classification\n",
    "\n",
    "On the 3rd task I hope to hear from my team. Or do some more research and implement some clean and simple algorithm.\n",
    "\n",
    "4th is just summarization."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
